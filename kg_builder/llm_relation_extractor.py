"""
LLM-based relation extraction module.
Uses large language models to extract complex and implicit relations
that pattern-based methods might miss.
"""

import json
import os
from typing import List, Dict, Optional
from openai import OpenAI

# LLM é…ç½®
# æ”¯æŒ CoreGPU DeepSeek-R1 API, OpenAI API, æœ¬åœ°æ¨¡åž‹ç­‰
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "coregpu")  # coregpu, openai, ollama
LLM_MODEL = os.getenv("LLM_MODEL", "DeepSeek-R1")  # CoreGPU ä½¿ç”¨ DeepSeek-R1
LLM_API_KEY = os.getenv("DPSK_API_KEY", "")
# æ³¨æ„ï¼šbase_url ä¸åŒ…å« /chat/completionsï¼ŒOpenAI SDK ä¼šè‡ªåŠ¨æ·»åŠ 
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "https://ai.api.coregpu.cn/v1")

# æ˜¯å¦å¯ç”¨ LLMï¼ˆé»˜è®¤å…³é—­ï¼Œéœ€è¦ API Key æ‰å¼€å¯ï¼‰
ENABLE_LLM = bool(LLM_API_KEY) and os.getenv("ENABLE_LLM_EXTRACTION", "false").lower() == "true"

# æ˜¯å¦å¯ç”¨æµå¼è¾“å‡ºï¼ˆç”¨äºŽè°ƒè¯•ï¼‰
ENABLE_STREAMING = os.getenv("ENABLE_LLM_STREAMING", "false").lower() == "true"


def get_llm_client():
    """
    èŽ·å– LLM å®¢æˆ·ç«¯ã€‚æ”¯æŒå¤šç§æä¾›å•†ã€‚
    """
    if not ENABLE_LLM:
        return None

    if LLM_PROVIDER == "coregpu":
        return OpenAI(
            api_key=LLM_API_KEY,
            base_url=LLM_BASE_URL
        )

    elif LLM_PROVIDER == "openai":
        return OpenAI(
            api_key=LLM_API_KEY,
            base_url="https://api.openai.com/v1"
        )

    elif LLM_PROVIDER == "ollama":
        return OpenAI(
            api_key="ollama",
            base_url="http://localhost:11434/v1"
        )

    return None



# Prompt æ¨¡æ¿
RELATION_EXTRACTION_PROMPT_ZH = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±æž„å»ºä¸“å®¶ï¼Œæ“…é•¿ä»Žä¸­æ–‡æŠ€æœ¯æ–‡æ¡£ä¸­æŠ½å–å®žä½“å…³ç³»ã€‚

ã€ä»»åŠ¡ã€‘
ä»Žä»¥ä¸‹æ–‡æœ¬ä¸­æŠ½å–æ‰€æœ‰æœ‰æ„ä¹‰çš„å®žä½“å…³ç³»ä¸‰å…ƒç»„ (ä¸»è¯­, å…³ç³», å®¾è¯­)ã€‚

ã€è¦æ±‚ã€‘
1. ä¸»è¯­å’Œå®¾è¯­åº”è¯¥æ˜¯å…·ä½“çš„å®žä½“ï¼ˆäººåã€ç»„ç»‡ã€ç³»ç»Ÿã€è®¾å¤‡ã€æ ‡å‡†ç­‰ï¼‰
2. å…³ç³»åº”è¯¥æ˜¯åŠ¨è¯æˆ–åŠ¨è¯çŸ­è¯­ï¼Œç®€æ´æ˜Žäº†
3. ä¼˜å…ˆæŠ½å–ä»¥ä¸‹ç±»åž‹çš„å…³ç³»ï¼š
   - ç»„æˆå…³ç³»ï¼ˆåŒ…å«ã€åŒ…æ‹¬ã€ç”±...ç»„æˆï¼‰
   - åŠŸèƒ½å…³ç³»ï¼ˆç”¨äºŽã€å®žçŽ°ã€è´Ÿè´£ã€æŽ§åˆ¶ï¼‰
   - è¿žæŽ¥å…³ç³»ï¼ˆè¿žæŽ¥ã€æŽ¥å£ã€é€šä¿¡ï¼‰
   - ä¾æ®å…³ç³»ï¼ˆåŸºäºŽã€ä¾æ®ã€ç¬¦åˆï¼‰
   - ç¼©å†™å…³ç³»ï¼ˆ...çš„ç¼©å†™ï¼‰
   - ä¼ è¾“å…³ç³»ï¼ˆå‘é€ã€æŽ¥æ”¶ã€ä¼ è¾“ï¼‰
4. å¿½ç•¥æ¨¡ç³Šçš„ã€ä¸ç¡®å®šçš„å…³ç³»
5. æ¯ä¸ªä¸‰å…ƒç»„ç‹¬ç«‹æˆè¡Œ

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›žï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« subject, relation, object ä¸‰ä¸ªå­—æ®µã€‚

ã€ç¤ºä¾‹ã€‘
è¾“å…¥ï¼šåŸŽå¸‚è½¨é“äº¤é€šä¿¡å·ç³»ç»Ÿ(CBTC)åŒ…æ‹¬åŒºåŸŸæŽ§åˆ¶å™¨(ZC)å’Œåˆ—è½¦è‡ªåŠ¨ç›‘æŽ§ç³»ç»Ÿ(ATS)ã€‚
è¾“å‡ºï¼š
```json
[
  {{"subject": "åŸŽå¸‚è½¨é“äº¤é€šä¿¡å·ç³»ç»Ÿ", "relation": "åŒ…æ‹¬", "object": "åŒºåŸŸæŽ§åˆ¶å™¨"}},
  {{"subject": "åŸŽå¸‚è½¨é“äº¤é€šä¿¡å·ç³»ç»Ÿ", "relation": "åŒ…æ‹¬", "object": "åˆ—è½¦è‡ªåŠ¨ç›‘æŽ§ç³»ç»Ÿ"}},
  {{"subject": "CBTC", "relation": "ç¼©å†™", "object": "åŸŽå¸‚è½¨é“äº¤é€šä¿¡å·ç³»ç»Ÿ"}},
  {{"subject": "ZC", "relation": "ç¼©å†™", "object": "åŒºåŸŸæŽ§åˆ¶å™¨"}},
  {{"subject": "ATS", "relation": "ç¼©å†™", "object": "åˆ—è½¦è‡ªåŠ¨ç›‘æŽ§ç³»ç»Ÿ"}}
]
```

ã€æ–‡æœ¬ã€‘
{{text}}

ã€è¾“å‡ºã€‘è¯·ç›´æŽ¥è¾“å‡ºJSONæ•°ç»„ï¼Œä¸è¦é¢å¤–è§£é‡Šï¼š"""

RELATION_EXTRACTION_PROMPT_EN = """You are an expert in knowledge graph construction, skilled at extracting entity relations from technical documents.

ã€Taskã€‘
Extract all meaningful entity relation triples (subject, relation, object) from the following text.

ã€Requirementsã€‘
1. Subject and object should be concrete entities (people, organizations, systems, equipment, standards, etc.)
2. Relation should be a verb or verb phrase, concise and clear
3. Prioritize these relation types:
   - Composition (contains, includes, composed of)
   - Functional (used for, implements, responsible for, controls)
   - Connection (connects, interfaces with, communicates)
   - Based on (based on, according to, complies with)
   - Abbreviation (abbreviation of)
   - Transmission (sends, receives, transmits)
4. Ignore vague or uncertain relations
5. Each triple on a separate line

ã€Output Formatã€‘
Return as a JSON array, each element with subject, relation, object fields.

ã€Exampleã€‘
Input: Apple, founded by Steve Jobs in 2003, is headquartered in California.
Output:
```json
[
  {{"subject": "Apple", "relation": "founded by", "object": "Steve Jobs"}},
  {{"subject": "Apple", "relation": "founded in", "object": "2003"}},
  {{"subject": "Apple", "relation": "headquartered in", "object": "California"}}
]
```

ã€Textã€‘
{{text}}

ã€Outputã€‘Return JSON array only, no extra explanation:"""


def extract_relations_with_llm(
    text: str,
    language: str = "zh",
    max_tokens: int = 2000,
    temperature: float = 0.1
) -> List[Dict[str, str]]:
    """
    ä½¿ç”¨ LLM ä»Žæ–‡æœ¬ä¸­æŠ½å–å…³ç³»ä¸‰å…ƒç»„ã€‚
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        language: è¯­è¨€ ('zh' æˆ– 'en')
        max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆè¶Šä½Žè¶Šç¡®å®šï¼‰
    
    Returns:
        å…³ç³»ä¸‰å…ƒç»„åˆ—è¡¨
    """
    if not ENABLE_LLM:
        return []
    
    client = get_llm_client()
    if not client:
        return []
    
    # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºå®žé™…é…ç½®
    print(f"ðŸ”§ Debug Info:")
    print(f"   Provider: {LLM_PROVIDER}")
    print(f"   Model: {LLM_MODEL}")
    print(f"   Base URL: {LLM_BASE_URL}")
    print(f"   API Key: {LLM_API_KEY[:10]}..." if LLM_API_KEY else "   API Key: Not set")
    print()
    
    # é€‰æ‹©åˆé€‚çš„ Prompt
    prompt_template = RELATION_EXTRACTION_PROMPT_ZH if language == "zh" else RELATION_EXTRACTION_PROMPT_EN
    prompt = prompt_template.format(text=text)
    
    try:
        # è°ƒç”¨ LLMï¼ˆæ”¯æŒæµå¼å’Œéžæµå¼ï¼‰
        if ENABLE_STREAMING:
            # æµå¼è¾“å‡ºï¼ˆç”¨äºŽè°ƒè¯•ï¼‰
            print("ðŸ”„ å¼€å§‹æµå¼è¾“å‡º...\n")
            response_stream = client.chat.completions.create(
                model=LLM_MODEL,
                messages=[
                    {"role": "system", "content": "You are a professional knowledge graph construction assistant. Always respond with valid JSON format."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                stream=True,
                response_format={"type": "json_object"}  # å¼ºåˆ¶ JSON è¾“å‡º
            )
            
            # æ”¶é›†æµå¼å“åº”
            full_content = ""
            reasoning_content = ""
            
            for chunk in response_stream:
                if not chunk.choices:
                    continue
                
                delta = chunk.choices[0].delta
                
                # æ”¶é›†æ€è€ƒè¿‡ç¨‹ï¼ˆDeepSeek-R1 ç‰¹æœ‰ï¼‰
                if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                    reasoning_content += delta.reasoning_content
                    print(delta.reasoning_content, end="", flush=True)
                
                # æ”¶é›†å®žé™…å†…å®¹
                if delta.content:
                    full_content += delta.content
            
            if reasoning_content:
                print("\n" + "="*50)
            
            content = full_content.strip()
        
        else:
            # éžæµå¼è¾“å‡ºï¼ˆæ­£å¸¸æ¨¡å¼ï¼‰
            response = client.chat.completions.create(
                model=LLM_MODEL,
                messages=[
                    {"role": "system", "content": "You are a professional knowledge graph construction assistant. Always respond with valid JSON format."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                response_format={"type": "json_object"}  # å¼ºåˆ¶ JSON è¾“å‡º
            )
            
            content = response.choices[0].message.content.strip()
        
        # è§£æžå“åº”
        
        # å°è¯•æå– JSONï¼ˆå¤„ç†å¯èƒ½çš„ markdown ä»£ç å—ï¼‰
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
        
        # è§£æž JSON
        relations = json.loads(content)
        
        # éªŒè¯æ ¼å¼
        if isinstance(relations, dict) and "relations" in relations:
            relations = relations["relations"]
        
        if not isinstance(relations, list):
            print(f"âš ï¸  LLM returned non-list format: {type(relations)}")
            return []
        
        # æ ‡å‡†åŒ–å­—æ®µå
        standardized = []
        for rel in relations:
            if isinstance(rel, dict):
                standardized.append({
                    "subject": rel.get("subject", rel.get("ä¸»è¯­", "")),
                    "relation": rel.get("relation", rel.get("å…³ç³»", "")),
                    "object": rel.get("object", rel.get("å®¾è¯­", ""))
                })
        
        return standardized
    
    except json.JSONDecodeError as e:
        print(f"âš ï¸  Failed to parse LLM response as JSON: {e}")
        print(f"    Response: {content[:200]}...")
        return []
    
    except Exception as e:
        print(f"âš ï¸  LLM extraction error: {e}")
        print(f"    Error type: {type(e).__name__}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ HTTP é”™è¯¯
        if hasattr(e, 'response'):
            print(f"    Status code: {e.response.status_code if hasattr(e.response, 'status_code') else 'unknown'}")
            print(f"    Response: {e.response.text[:200] if hasattr(e.response, 'text') else 'no response text'}...")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ OpenAI API é”™è¯¯
        if hasattr(e, 'body'):
            print(f"    Error body: {e.body}")
        
        import traceback
        print(f"    Traceback:")
        traceback.print_exc()
        
        return []


def extract_relations_batch_with_llm(
    texts: List[str],
    language: str = "zh",
    batch_size: int = 5
) -> List[List[Dict[str, str]]]:
    """
    æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æœ¬ã€‚
    
    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
        language: è¯­è¨€
        batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆä¸€æ¬¡å¤„ç†å‡ ä¸ªå¥å­ï¼‰
    
    Returns:
        æ¯ä¸ªæ–‡æœ¬å¯¹åº”çš„å…³ç³»åˆ—è¡¨
    """
    if not ENABLE_LLM:
        return [[] for _ in texts]
    
    results = []
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        # åˆå¹¶æ–‡æœ¬
        combined_text = "\n\n".join([f"[æ®µè½{j+1}] {t}" for j, t in enumerate(batch)])
        
        # æå–å…³ç³»
        relations = extract_relations_with_llm(combined_text, language)
        
        # TODO: æ›´æ™ºèƒ½çš„åˆ†é…å…³ç³»åˆ°å¯¹åº”å¥å­
        # çŽ°åœ¨ç®€å•åœ°æŠŠæ‰€æœ‰å…³ç³»éƒ½åˆ†é…ç»™ç¬¬ä¸€ä¸ªæ–‡æœ¬
        results.append(relations)
        for _ in range(len(batch) - 1):
            results.append([])
    
    return results


if __name__ == "__main__":
    # æµ‹è¯•
    print("="*70)
    print("LLM Relation Extraction Test")
    print("="*70)
    
    if not ENABLE_LLM:
        print("\nâš ï¸  LLM extraction is DISABLED.")
        print("To enable:")
        print("  1. Set environment variable: export DPSK_API_KEY='your-deepseek-key'")
        print("  2. Set environment variable: export ENABLE_LLM_EXTRACTION='true'")
        print("  3. (Optional) Set LLM_MODEL, LLM_PROVIDER, LLM_BASE_URL")
        print("\nðŸ’¡ Quick setup:")
        print("  python setup_deepseek.py")
    else:
        print(f"\nâœ“ LLM enabled: {LLM_PROVIDER} / {LLM_MODEL}")
        print(f"âœ“ Base URL: {LLM_BASE_URL}")
        
        test_text = """
        åŸŽå¸‚è½¨é“äº¤é€šä¿¡å·ç³»ç»Ÿ(CBTC)åŒ…æ‹¬åŒºåŸŸæŽ§åˆ¶å™¨(ZC)ã€åˆ—è½¦è‡ªåŠ¨ç›‘æŽ§ç³»ç»Ÿ(ATS)å’Œè½¦è½½æŽ§åˆ¶å™¨(VOBC)ã€‚
        ZCè´Ÿè´£åˆ—è½¦è¿è¡ŒæŽ§åˆ¶ï¼Œé€šè¿‡RSSPåè®®ä¸ŽATSè¿›è¡Œå®‰å…¨é€šä¿¡ã€‚
        ç³»ç»Ÿç¬¦åˆGB/T 28807-2012æ ‡å‡†ã€‚
        """
        
        print(f"\nTest Text:\n{test_text}\n")
        
        relations = extract_relations_with_llm(test_text, language="zh")
        
        print(f"Extracted {len(relations)} relations:")
        for rel in relations:
            print(f"  ({rel['subject']}) --[{rel['relation']}]--> ({rel['object']})")

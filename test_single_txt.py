#!/usr/bin/env python3
"""
æµ‹è¯•å•ä¸ªæ–‡æœ¬æ–‡ä»¶çš„çŸ¥è¯†å›¾è°±æ„å»º
ä½¿ç”¨ LLM è¿›è¡Œå…³ç³»æŠ½å–
"""

import os
import sys
import json

# æ·»åŠ  kg_builder åˆ°è·¯å¾„
sys.path.append('kg_builder')

def test_single_file(filename: str):
    """
    æµ‹è¯•å¤„ç†å•ä¸ªæ–‡æœ¬æ–‡ä»¶
    
    Args:
        filename: æ–‡ä»¶åï¼ˆç›¸å¯¹äº texts/ ç›®å½•ï¼‰
    """
    from text_preprocessing import preprocess_document
    from language_utils import detect_language
    from ner_chinese import extract_chinese_entities
    from candidate_filter import filter_candidate_sentences
    from llm_relation_extractor import extract_relations_with_llm
    from graph_builder import build_graph_from_triples
    
    import spacy
    try:
        nlp = spacy.load("en_core_web_sm")
    except:
        print("âš ï¸  spaCy æ¨¡å‹æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•çš„å¥å­åˆ†å‰²")
        nlp = None
    
    print("="*70)
    print(f"æµ‹è¯•æ–‡ä»¶: {filename}")
    print("="*70)
    
    # è¯»å–æ–‡ä»¶
    file_path = os.path.join('texts', filename)
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    print(f"\nğŸ“„ æ–‡ä»¶ä¿¡æ¯:")
    print(f"  æ–‡ä»¶å¤§å°: {len(text)} å­—ç¬¦")
    print(f"  å‰ 200 å­—ç¬¦: {text[:200]}...")
    
    # Stage 1: è¯­è¨€æ£€æµ‹
    print(f"\n[1/6] æ£€æµ‹è¯­è¨€...")
    language = detect_language(text)
    print(f"  â†’ è¯­è¨€: {'ä¸­æ–‡' if language == 'zh' else 'è‹±æ–‡'}")
    
    # Stage 2: æ–‡æœ¬é¢„å¤„ç†
    print(f"\n[2/6] é¢„å¤„ç†æ–‡æœ¬...")
    cleaned_text = preprocess_document(text)
    print(f"  â†’ æ¸…ç†å: {len(cleaned_text)} å­—ç¬¦")
    
    # Stage 3: å¥å­åˆ†å‰²
    print(f"\n[3/6] åˆ†å‰²å¥å­...")
    if nlp:
        doc = nlp(cleaned_text[:1000000])  # é™åˆ¶é•¿åº¦é¿å…å†…å­˜é—®é¢˜
        sentences = [sent.text.strip() for sent in doc.sents]
    else:
        # ç®€å•çš„å¥å­åˆ†å‰²
        sentences = [s.strip() for s in cleaned_text.split('ã€‚') if s.strip()]
    
    sentences = [s for s in sentences if len(s) > 10]
    print(f"  â†’ {len(sentences)} ä¸ªå¥å­")
    
    # Stage 4: å€™é€‰å¥å­è¿‡æ»¤
    print(f"\n[4/6] è¿‡æ»¤å€™é€‰å¥å­...")
    candidates = filter_candidate_sentences(sentences, language)
    filter_rate = (1 - len(candidates)/len(sentences)) * 100 if sentences else 0
    print(f"  â†’ {len(candidates)} ä¸ªå€™é€‰å¥å­ (è¿‡æ»¤ç‡: {filter_rate:.1f}%)")
    
    if candidates:
        print(f"\n  ç¤ºä¾‹å€™é€‰å¥å­ (å‰3ä¸ª):")
        for i, sent in enumerate(candidates[:3], 1):
            print(f"    {i}. {sent[:80]}...")
    
    # Stage 5: LLM å…³ç³»æŠ½å–
    print(f"\n[5/6] LLM å…³ç³»æŠ½å–...")
    
    # é™åˆ¶å€™é€‰å¥å­æ•°é‡ä»¥æ§åˆ¶æˆæœ¬
    max_candidates = 20
    if len(candidates) > max_candidates:
        print(f"  âš ï¸  å€™é€‰å¥å­è¿‡å¤šï¼Œä»…å¤„ç†å‰ {max_candidates} ä¸ª")
        candidates = candidates[:max_candidates]
    
    all_triples = []
    batch_size = 5
    
    for i in range(0, len(candidates), batch_size):
        batch = candidates[i:i+batch_size]
        print(f"  â†’ å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(candidates)-1)//batch_size + 1} ({len(batch)} å¥)...")
        
        # åˆå¹¶æ‰¹æ¬¡
        combined_text = "\n\n".join(batch)
        
        # æå–å…³ç³»
        triples = extract_relations_with_llm(combined_text, language=language)
        all_triples.extend(triples)
        
        print(f"    âœ“ æå– {len(triples)} ä¸ªå…³ç³»")
    
    print(f"\n  æ€»è®¡: {len(all_triples)} ä¸ªå…³ç³»ä¸‰å…ƒç»„")
    
    # Stage 6: æ„å»ºçŸ¥è¯†å›¾è°±
    print(f"\n[6/6] æ„å»ºçŸ¥è¯†å›¾è°±...")
    
    # æå–å®ä½“
    entities = {}
    for sent in sentences[:100]:  # é™åˆ¶å¤„ç†çš„å¥å­æ•°
        ents = extract_chinese_entities(sent) if language == 'zh' else []
        for ent in ents:
            if ent['text'] not in entities:
                entities[ent['text']] = ent['type']
    
    print(f"  â†’ æå– {len(entities)} ä¸ªå®ä½“")
    
    # æ„å»ºå›¾
    kg = build_graph_from_triples(all_triples, entities)
    
    # æ˜¾ç¤ºç»Ÿè®¡
    stats = kg.get_statistics()
    print(f"\nğŸ“Š çŸ¥è¯†å›¾è°±ç»Ÿè®¡:")
    print(f"  å®ä½“æ•°: {stats['num_entities']}")
    print(f"  å…³ç³»æ•°: {stats['num_relations']}")
    print(f"  å…³ç³»ç±»å‹æ•°: {stats['num_relation_types']}")
    print(f"  å…³ç³»ç±»å‹: {', '.join(stats['relation_types'][:10])}")
    
    # æ˜¾ç¤ºç¤ºä¾‹å…³ç³»
    if all_triples:
        print(f"\nğŸ“‹ ç¤ºä¾‹å…³ç³» (å‰10ä¸ª):")
        for i, triple in enumerate(all_triples[:10], 1):
            print(f"  {i}. ({triple['subject']}) --[{triple['relation']}]--> ({triple['object']})")
    
    # ä¿å­˜ç»“æœ
    output_dir = 'output'
    os.makedirs(output_dir, exist_ok=True)
    
    # å®‰å…¨çš„æ–‡ä»¶å
    safe_filename = filename.replace('/', '_').replace('\\', '_')
    output_path = os.path.join(output_dir, f"kg_{safe_filename}.json")
    
    kg.save(output_path)
    
    print(f"\nâœ… å®Œæˆ!")
    print(f"  ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print("="*70)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python test_single_txt.py <æ–‡ä»¶å>")
        print("\nå¯ç”¨æ–‡ä»¶:")
        
        texts_dir = 'texts'
        if os.path.exists(texts_dir):
            files = [f for f in os.listdir(texts_dir) if f.endswith('.txt')]
            for i, f in enumerate(files[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  {i}. {f}")
            if len(files) > 5:
                print(f"  ... è¿˜æœ‰ {len(files)-5} ä¸ªæ–‡ä»¶")
        
        print("\nç¤ºä¾‹:")
        print("  python test_single_txt.py 'ä¿®æ”¹å•-TCAMET 04010.1â€”2018ã€Šâ€¦â€¦äº’è”äº’é€šç³»ç»Ÿè§„èŒƒ ç¬¬1éƒ¨åˆ†ï¼šç³»ç»Ÿæ€»ä½“è¦æ±‚ã€‹.txt'")
        sys.exit(1)
    
    filename = sys.argv[1]
    test_single_file(filename)
